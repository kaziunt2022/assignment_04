{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/unt-iialab/INFO5731_Spring2020/blob/master/Assignments/INFO5731_Assignment_Four.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "USSdXHuqnwv9"
   },
   "source": [
    "# **INFO5731 Assignment Four**\n",
    "\n",
    "In this assignment, you are required to conduct topic modeling, sentiment analysis based on **the dataset you created from assignment three**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YWxodXh5n4xF"
   },
   "source": [
    "# **Question 1: Topic Modeling**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TenBkDJ5n95k"
   },
   "source": [
    "(30 points). This question is designed to help you develop a feel for the way topic modeling works, the connection to the human meanings of documents. Based on the dataset from assignment three, write a python program to **identify the top 10 topics in the dataset**. Before answering this question, please review the materials in lesson 8, especially the code for LDA and LSA. The following information should be reported:\n",
    "\n",
    "(1) Features (top n-gram phrases) used for topic modeling.\n",
    "\n",
    "(2) Top 10 clusters for topic modeling.\n",
    "\n",
    "(3) Summarize and describe the topic for each cluster. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PuFPKhC0m1fd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import librariess\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import re, string, nltk, spacy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn import model_selection\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import svm\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from gensim import corpora, models, utils\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Reviews.CSV\") # read the reviews file from asignment 3\n",
    "df.dropna(subset=[\"Reviews\"], inplace=True) # drop null rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cleaning the reviews\n",
    "data = df.Reviews.values.tolist() # Convert each review to list\n",
    "data = [re.sub('\\s+', ' ', sentence) for sentence in data] # remose the line breakers\n",
    "data = [re.sub(\"\\'\",\" \", sentence) for sentence in data] # remocve the \\'\n",
    "\n",
    "def sent_to_words(reviews):\n",
    "    \"\"\"\n",
    "    Input: sentence--> string\n",
    "    Function: Tokenize the sentence and remove punctuations\n",
    "    Output: tokenize and clean reviews\n",
    "    \"\"\"\n",
    "    sentence = []\n",
    "    for review in reviews:\n",
    "        sentence.append(utils.simple_preprocess(str(review).encode('utf-8'), deacc=True))  # deacc=True removes punctuations\n",
    "    return sentence\n",
    "tokenize_reviews = list(sent_to_words(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "## bigram and trigam mmodels \n",
    "bigram = models.Phrases(tokenize_reviews, min_count=5, threshold=100) # creat bigram phrases\n",
    "bigram_model = models.phrases.Phraser(bigram) # bigram model\n",
    "trigram_model = models.phrases.Phraser(models.Phrases(bigram[tokenize_reviews], threshold=100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions for stopwords, bigrams, trigrams and lemmatization\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])\n",
    "def remove_stopwords(reviews): \n",
    "    \"\"\"\n",
    "    Input: list of lists of reviews\n",
    "    Func: remove all stopwords\n",
    "    Output: tokenize reviews without stop words\n",
    "    \"\"\"\n",
    "    return [[word for word in utils.simple_preprocess(str(review)) if word not in stop_words] for review in reviews]\n",
    "\n",
    "def make_bigrams(reviews):\n",
    "    \"\"\"\n",
    "    Input: tokenize reviews\n",
    "    Func: make bigrams\n",
    "    Output: bigrams of reviews\n",
    "    \"\"\"\n",
    "    return [bigram_model[review] for review in reviews]\n",
    "\n",
    "def make_trigrams(reviews):\n",
    "    \"\"\"\n",
    "    Input: tokenize reviews\n",
    "    Func: make trigrams\n",
    "    Output: trigrams of bigram reviews\n",
    "    \"\"\"\n",
    "    return [trigram_model[bigram_model[review]] for review in reviews]\n",
    "\n",
    "def lemmatization(reviews, allowed=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"\n",
    "    Input: tokenize bigram reviews\n",
    "    Func: return only Noun, adj, verb, adverbs\n",
    "    Output: nouns, adj, verb, adv of reviews\n",
    "    \"\"\"\n",
    "    output_reviews= []\n",
    "    for sent in reviews:\n",
    "        review = nlp(\" \".join(sent)) \n",
    "        output_reviews.append([token.lemma_ for token in review if token.pos_ in allowed])\n",
    "    return output_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrame_reviews = make_bigrams(remove_stopwords(tokenize_reviews)) # take bigram of the Reviews without stopwords\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner']) # initiaize the nlp english model\n",
    "lemmatize_reviews = lemmatization(bigrame_reviews, ['NOUN', 'ADJ', 'VERB', 'ADV']) # nouns, adj, verb, adv of reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word = corpora.Dictionary(lemmatize_reviews) # Create Dictionary\n",
    "corpus = [id2word.doc2bow(review) for review in lemmatize_reviews] # freq of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics = 10, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True) # create LDA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.043*\"familiar\" + 0.025*\"serious\" + 0.024*\"weapon\" + 0.024*\"pacing\" + 0.020*\"min\" + 0.019*\"flaw\" + 0.015*\"finish\" + 0.013*\"difference\" + 0.012*\"comedic_time\" + 0.009*\"overdone\"'),\n",
       " (1,\n",
       "  '0.021*\"storyline\" + 0.015*\"hope\" + 0.014*\"usual\" + 0.012*\"leave\" + 0.010*\"theme\" + 0.010*\"man\" + 0.010*\"amount\" + 0.009*\"attempt\" + 0.009*\"recent\" + 0.009*\"year\"'),\n",
       " (2,\n",
       "  '0.029*\"invest\" + 0.027*\"doubt\" + 0.021*\"expand\" + 0.015*\"casting\" + 0.012*\"visually_stunning\" + 0.009*\"concern\" + 0.000*\"underdeveloped\" + 0.000*\"priority\" + 0.000*\"loyal\" + 0.000*\"sanity\"'),\n",
       " (3,\n",
       "  '0.000*\"reunion\" + 0.000*\"positively\" + 0.000*\"bollywood\" + 0.000*\"biopic\" + 0.000*\"useful\" + 0.000*\"curate\" + 0.000*\"album\" + 0.000*\"forever\" + 0.000*\"ye\" + 0.000*\"stoic\"'),\n",
       " (4,\n",
       "  '0.026*\"middle\" + 0.021*\"wenwu\" + 0.020*\"shaun\" + 0.016*\"ta_lo\" + 0.014*\"young\" + 0.014*\"attack\" + 0.014*\"dweller\" + 0.012*\"death\" + 0.012*\"katy\" + 0.011*\"pendant\"'),\n",
       " (5,\n",
       "  '0.040*\"forgettable\" + 0.026*\"version\" + 0.021*\"meh\" + 0.008*\"fabulous\" + 0.004*\"pretend\" + 0.003*\"toe\" + 0.001*\"teller\" + 0.000*\"bull\" + 0.000*\"af\" + 0.000*\"sht\"'),\n",
       " (6,\n",
       "  '0.040*\"hold\" + 0.034*\"mess\" + 0.019*\"captain\" + 0.017*\"ending\" + 0.007*\"country\" + 0.006*\"repetitive\" + 0.005*\"hustle\" + 0.005*\"cake\" + 0.004*\"jazz\" + 0.003*\"adventurous\"'),\n",
       " (7,\n",
       "  '0.035*\"film\" + 0.012*\"also\" + 0.010*\"character\" + 0.010*\"ten_ring\" + 0.008*\"first\" + 0.008*\"find\" + 0.007*\"feel\" + 0.006*\"get\" + 0.006*\"take\" + 0.006*\"become\"'),\n",
       " (8,\n",
       "  '0.056*\"movie\" + 0.030*\"marvel\" + 0.023*\"good\" + 0.018*\"character\" + 0.018*\"great\" + 0.018*\"scene\" + 0.016*\"story\" + 0.015*\"well\" + 0.015*\"see\" + 0.014*\"action\"'),\n",
       " (9,\n",
       "  '0.031*\"issue\" + 0.023*\"steal\" + 0.018*\"break\" + 0.017*\"unnecessary\" + 0.016*\"pure\" + 0.016*\"struggle\" + 0.014*\"heavy\" + 0.013*\"carry\" + 0.013*\"gang\" + 0.012*\"suddenly\"')]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_model.print_topics() # print lda topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsa_model = models.LsiModel(corpus, num_topics = 10, id2word = id2word) # create lsa model over corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.466*\"movie\" + 0.353*\"film\" + 0.256*\"marvel\" + 0.237*\"character\" + 0.189*\"well\" + 0.188*\"good\" + 0.170*\"scene\" + 0.153*\"great\" + 0.141*\"action\" + 0.140*\"fight\"'),\n",
       " (1,\n",
       "  '0.697*\"movie\" + -0.615*\"film\" + -0.129*\"also\" + -0.128*\"well\" + -0.112*\"character\" + 0.083*\"marvel\" + -0.074*\"mcu\" + 0.064*\"good\" + 0.061*\"watch\" + -0.060*\"great\"'),\n",
       " (2,\n",
       "  '-0.515*\"film\" + -0.351*\"marvel\" + 0.216*\"character\" + -0.202*\"movie\" + 0.200*\"go\" + 0.192*\"also\" + 0.144*\"really\" + 0.129*\"fight\" + 0.124*\"find\" + 0.123*\"think\"'),\n",
       " (3,\n",
       "  '-0.552*\"marvel\" + 0.358*\"mcu\" + 0.305*\"great\" + 0.199*\"scene\" + 0.186*\"good\" + 0.160*\"movie\" + 0.139*\"really\" + -0.136*\"get\" + -0.119*\"make\" + 0.104*\"also\"'),\n",
       " (4,\n",
       "  '-0.611*\"good\" + 0.289*\"well\" + 0.213*\"character\" + 0.210*\"movie\" + -0.205*\"marvel\" + 0.175*\"feel\" + -0.152*\"fight\" + -0.133*\"wenwu\" + -0.114*\"mcu\" + -0.108*\"shaun\"'),\n",
       " (5,\n",
       "  '0.368*\"great\" + -0.339*\"film\" + 0.311*\"marvel\" + 0.259*\"really\" + -0.243*\"movie\" + 0.204*\"character\" + 0.199*\"action\" + 0.165*\"story\" + -0.165*\"go\" + 0.164*\"good\"'),\n",
       " (6,\n",
       "  '0.452*\"mcu\" + -0.364*\"really\" + 0.318*\"great\" + -0.277*\"good\" + -0.238*\"scene\" + 0.148*\"new\" + 0.147*\"story\" + 0.146*\"action\" + 0.140*\"ten_ring\" + -0.138*\"also\"'),\n",
       " (7,\n",
       "  '-0.371*\"scene\" + -0.361*\"fight\" + 0.324*\"really\" + 0.256*\"mcu\" + -0.246*\"great\" + 0.217*\"feel\" + -0.208*\"action\" + -0.206*\"well\" + 0.174*\"character\" + 0.169*\"story\"'),\n",
       " (8,\n",
       "  '-0.324*\"also\" + 0.310*\"great\" + 0.272*\"feel\" + 0.261*\"fight\" + 0.260*\"story\" + -0.221*\"well\" + -0.192*\"movie\" + 0.191*\"scene\" + -0.181*\"good\" + -0.174*\"action\"'),\n",
       " (9,\n",
       "  '0.365*\"scene\" + -0.311*\"good\" + -0.281*\"great\" + 0.253*\"character\" + 0.205*\"marvel\" + 0.190*\"new\" + 0.185*\"mcu\" + -0.177*\"make\" + -0.177*\"story\" + -0.164*\"give\"')]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsa_model.print_topics() # print lsa topics with key words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LDA:\n",
      "    1)Familia Series with weapons and min flaw\n",
      "    2)Hope in storyline Theme\n",
      "    3)Expanding Casting\n",
      "    4)Positve anf useful reunion album\n",
      "    5)Attack and Young man's death\n",
      "    6)Fabulous version\n",
      "    7)Adventures Ending\n",
      "    8)Feel of film characters\n",
      "    9)Great marvel movie with story and action\n",
      "    10)Uneccessary steel breaking\n",
      "\n",
      "LSA:\n",
      "    1)Marveel Movies are great action movies\n",
      "    2)Marvel movie is also well charactered\n",
      "    3)Marevel movies go for real fight\n",
      "    4)Marvel movies has realy great scenes\n",
      "    5)Good and well characted movie\n",
      "    6)Marvel film with action story\n",
      "    7)Good and new action scenes\n",
      "    8)Characters with real fighting scenes\n",
      "    9)Movie is well actioned with great fight scenes\n",
      "    10)Good scenes with new characters\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "# print out topics visioned by lda and lsa\n",
    "lda_topics =\"\"\"\n",
    "LDA:\n",
    "    1)Familia Series with weapons and min flaw\n",
    "    2)Hope in storyline Theme\n",
    "    3)Expanding Casting\n",
    "    4)Positve anf useful reunion album\n",
    "    5)Attack and Young man's death\n",
    "    6)Fabulous version\n",
    "    7)Adventures Ending\n",
    "    8)Feel of film characters\n",
    "    9)Great marvel movie with story and action\n",
    "    10)Uneccessary steel breaking\"\"\"\n",
    "lsa_topics = \"\"\"\n",
    "LSA:\n",
    "    1)Marveel Movies are great action movies\n",
    "    2)Marvel movie is also well charactered\n",
    "    3)Marevel movies go for real fight\n",
    "    4)Marvel movies has realy great scenes\n",
    "    5)Good and well characted movie\n",
    "    6)Marvel film with action story\n",
    "    7)Good and new action scenes\n",
    "    8)Characters with real fighting scenes\n",
    "    9)Movie is well actioned with great fight scenes\n",
    "    10)Good scenes with new characters\n",
    "    \"\"\"\n",
    "print(lda_topics)\n",
    "print(lsa_topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AfpMRCrRwN6Z"
   },
   "source": [
    "# **Question 2: Sentiment Analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1dCQEbDawWCw"
   },
   "source": [
    "(30 points). Sentiment analysis also known as opinion mining is a sub field within Natural Language Processing (NLP) that builds machine learning algorithms to classify a text according to the sentimental polarities of opinions it contains, e.g., positive, negative, neutral. The purpose of this question is to develop a machine learning classifier for sentiment analysis. Based on the dataset from assignment three, write a python program to implement a sentiment classifier and evaluate its performance. Notice: **80% data for training and 20% data for testing**.  \n",
    "\n",
    "(1) Features used for sentiment classification and explain why you select these features.\n",
    "\n",
    "(2) Select two of the supervised learning algorithm from scikit-learn library: https://scikit-learn.org/stable/supervised_learning.html#supervised-learning, to build a sentiment classifier respectively. \n",
    "\n",
    "(3) Compare the performance over accuracy, precision, recall, and F1 score for the two algorithms you selected. Here is the reference of how to calculate these metrics: https://towardsdatascience.com/accuracy-precision-recall-or-f1-331fb37c5cb9. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vATjQNTY8buA"
   },
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "def review_classification(rating):\n",
    "    if rating == 5: return 'Very Positive'\n",
    "    elif rating == 4: return 'Positive'\n",
    "    elif rating == 3: return 'Neutral'\n",
    "    elif rating == 2: return 'Negative'\n",
    "    elif rating == 1: return 'Very Negative'\n",
    "\n",
    "df = pd.read_csv(\"Amazon Reviews.CSV\")\n",
    "df.dropna(inplace=True, subset=['Rating', \"Reviews\"])\n",
    "df[\"Sentiment\"] = df[\"Rating\"].apply(review_classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_list = stopwords.words('english') # importing stopwords\n",
    "punctuations_list = string.punctuation # get punctuations\n",
    "lemmatizer = WordNetLemmatizer() # initialize word lemmatizer\n",
    "def preprocessing(text):\n",
    "    \"\"\"\n",
    "    This function will clean the givern text\n",
    "    \"\"\"\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text)\n",
    "    text = re.sub('[\\W]+', ' ', text.lower())\n",
    "    text = text + \" \".join(emoticons).replace('-', '')\n",
    "    tokenize_text = [lemmatizer.lemmatize(word.lower()) for word in nltk.tokenize.word_tokenize(text) if (word not in stopwords_list) and (word not in punctuations_list) and (len(word)>=2) and (word.isalnum())]\n",
    "    return \" \".join(tokenize_text)\n",
    "\n",
    "df[\"Clean Review\"] = df[\"Reviews\"].apply(preprocessing) # Clean all the reviews got from txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tf_idf = TfidfVectorizer(ngram_range=(1,2), max_features=1000)\n",
    "tf_idf.fit(df['Clean Review'])\n",
    "X = tf_idf.transform(df['Clean Review'])\n",
    "encoder = LabelEncoder()\n",
    "y = encoder.fit_transform(df['Rating'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Accuracy:  0.7072274397713353\n",
      "Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.85      0.76       531\n",
      "           1       0.46      0.15      0.23       149\n",
      "           2       0.51      0.25      0.34       193\n",
      "           3       0.50      0.17      0.25       331\n",
      "           4       0.75      0.92      0.83      1245\n",
      "\n",
      "    accuracy                           0.71      2449\n",
      "   macro avg       0.58      0.47      0.48      2449\n",
      "weighted avg       0.67      0.71      0.66      2449\n",
      "\n"
     ]
    }
   ],
   "source": [
    "svm_model = svm.SVC(kernel='linear')\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "predicted_y = svm_model.predict(X_test)\n",
    "svm_report = classification_report(y_test, predicted_y, output_dict = True)\n",
    "\n",
    "print(\"SVM Accuracy: \", accuracy_score(y_test, predicted_y))\n",
    "print(\"Classification Report: \")\n",
    "print(classification_report(y_test, predicted_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Model Accuracy:  0.7419354838709677\n",
      "Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.78      0.78       531\n",
      "           1       0.51      0.53      0.52       149\n",
      "           2       0.58      0.53      0.55       193\n",
      "           3       0.56      0.50      0.53       331\n",
      "           4       0.82      0.85      0.83      1245\n",
      "\n",
      "    accuracy                           0.74      2449\n",
      "   macro avg       0.65      0.64      0.64      2449\n",
      "weighted avg       0.74      0.74      0.74      2449\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tree_model = DecisionTreeClassifier()\n",
    "tree_model.fit(X_train, y_train)\n",
    "predicted_y = tree_model.predict(X_test)\n",
    "tree_report = classification_report(y_test, predicted_y, output_dict = True)\n",
    "\n",
    "print(\"Decision Tree Model Accuracy: \", accuracy_score(y_test, predicted_y))\n",
    "print(\"Classification Report: \")\n",
    "print(classification_report(y_test, predicted_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0': {'precision': 0.7849056603773585,\n",
       "  'recall': 0.783427495291902,\n",
       "  'f1-score': 0.7841658812441094,\n",
       "  'support': 531},\n",
       " '1': {'precision': 0.5064102564102564,\n",
       "  'recall': 0.5302013422818792,\n",
       "  'f1-score': 0.5180327868852459,\n",
       "  'support': 149},\n",
       " '2': {'precision': 0.5828571428571429,\n",
       "  'recall': 0.5284974093264249,\n",
       "  'f1-score': 0.5543478260869565,\n",
       "  'support': 193},\n",
       " '3': {'precision': 0.5622895622895623,\n",
       "  'recall': 0.5045317220543807,\n",
       "  'f1-score': 0.5318471337579618,\n",
       "  'support': 331},\n",
       " '4': {'precision': 0.8156467854376452,\n",
       "  'recall': 0.8457831325301205,\n",
       "  'f1-score': 0.8304416403785488,\n",
       "  'support': 1245},\n",
       " 'accuracy': 0.7419354838709677,\n",
       " 'macro avg': {'precision': 0.6504218814743931,\n",
       "  'recall': 0.6384882202969415,\n",
       "  'f1-score': 0.6437670536705644,\n",
       "  'support': 2449},\n",
       " 'weighted avg': {'precision': 0.7375784219782147,\n",
       "  'recall': 0.7419354838709677,\n",
       "  'f1-score': 0.7392851540083646,\n",
       "  'support': 2449}}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"SVM Accuracy: \", svm[\"accuracy\"], '\\tvs\\t', \"Decision Tree Accuracy: \", tree_report[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Accuracy: 0.71 \tvs\t Decision Tree Accuracy: 0.74\n",
      "SVM Prescision: 0.67 \tvs\t Decision Tree Accuracy: 0.74\n",
      "SVM Prescision: 0.71 \tvs\t Decision Tree Accuracy: 0.74\n",
      "SVM Prescision: 0.66 \tvs\t Decision Tree Accuracy: 0.74\n"
     ]
    }
   ],
   "source": [
    "print(\"SVM Accuracy:\", round(svm_report[\"accuracy\"], 2), '\\tvs\\t', \n",
    "      \"Decision Tree Accuracy:\", round(tree_report[\"accuracy\"], 2))\n",
    "print(\"SVM Prescision:\", round(svm_report[\"weighted avg\"][\"precision\"], 2), '\\tvs\\t', \n",
    "      \"Decision Tree Accuracy:\", round(tree_report[\"weighted avg\"][\"precision\"], 2))\n",
    "print(\"SVM Prescision:\", round(svm_report[\"weighted avg\"][\"recall\"], 2), '\\tvs\\t', \n",
    "      \"Decision Tree Accuracy:\", round(tree_report[\"weighted avg\"][\"recall\"], 2))\n",
    "print(\"SVM Prescision:\", round(svm_report[\"weighted avg\"][\"f1-score\"], 2), '\\tvs\\t', \n",
    "      \"Decision Tree Accuracy:\", round(tree_report[\"weighted avg\"][\"f1-score\"], 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E5mmYIfN8eYV"
   },
   "source": [
    "# **Question 3: House price prediction**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hsi2y4z88ngX"
   },
   "source": [
    "(40 points). You are required to build a **regression** model to predict the house price with 79 explanatory variables describing (almost) every aspect of residential homes. The purpose of this question is to practice regression analysis, an supervised learning model. The training data, testing data, and data description files can be download here: https://github.com/unt-iialab/info5731_spring2021/blob/main/assignment/assignment4-question3-data.zip. Here is an axample for implementation: https://towardsdatascience.com/linear-regression-in-python-predict-the-bay-areas-home-price-5c91c8378878. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XfvMKJjIXS5G"
   },
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "train_data = pd.read_csv(\"train.csv\")\n",
    "test_data = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\AppData\\Local\\Temp/ipykernel_9788/3683353847.py:1: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  train_data.fillna(train_data.mean(), inplace = True)\n",
      "C:\\Users\\Dell\\AppData\\Local\\Temp/ipykernel_9788/3683353847.py:2: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  test_data.fillna(test_data.mean(), inplace = True)\n"
     ]
    }
   ],
   "source": [
    "train_data.fillna(train_data.mean(), inplace = True)\n",
    "test_data.fillna(test_data.mean(), inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "columns = ('GarageCond', 'LandContour', 'RoofStyle', 'RoofMatl', 'Heating', 'MiscFeature', 'SaleType', 'GarageType', 'Electrical', \n",
    "           'SaleCondition', 'Foundation', 'Exterior1st', 'Exterior2nd', 'MasVnrType', 'FireplaceQu', 'LotConfig', 'Neighborhood', \n",
    "           'Condition1', 'Condition2', 'Utilities', 'BldgType', 'HouseStyle','PoolQC', 'BsmtQual', 'BsmtCond', 'GarageQual',\n",
    "           'BsmtExposure', 'ExterQual', 'ExterCond','HeatingQC', 'KitchenQual', 'BsmtFinType1','BsmtFinType2', 'Functional',\n",
    "           'Fence', 'GarageFinish', 'LandSlope','LotShape', 'PavedDrive', 'Street', 'Alley', 'CentralAir', 'MSSubClass',\n",
    "           'OverallCond', 'YrSold', 'MoSold', 'MSZoning')\n",
    "\n",
    "for column in columns:\n",
    "    encoder = LabelEncoder()\n",
    "    train_data[column] = encoder.fit(list(train_data[column].values)).transform(list(train_data[column].values))\n",
    "for column in columns:\n",
    "    encoder = LabelEncoder()\n",
    "    test_data[column] = encoder.fit(list(test_data[column].values)).transform(list(test_data[column].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_data[train_data.columns[:80]]\n",
    "y_train = train_data['SalePrice']\n",
    "X_test = test_data[test_data.columns[:80]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "rg_model = LinearRegression()\n",
    "rg_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_perdicted = rg_model.predict(X_test)\n",
    "fill_data = pd.read_csv(\"test.csv\")\n",
    "fill_data[\"Perdicted Price\"] = y_perdicted\n",
    "fill_data.to_csv(\"Perdicted Prices.CSV\", index=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "INFO5731_Assignment_Three.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
