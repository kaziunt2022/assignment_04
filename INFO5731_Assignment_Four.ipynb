{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "INFO5731_Assignment_Three.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kaziunt2022/assignment_04/blob/main/INFO5731_Assignment_Four.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USSdXHuqnwv9"
      },
      "source": [
        "# **INFO5731 Assignment Four**\n",
        "\n",
        "In this assignment, you are required to conduct topic modeling, sentiment analysis based on **the dataset you created from assignment three**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWxodXh5n4xF"
      },
      "source": [
        "# **Question 1: Topic Modeling**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TenBkDJ5n95k"
      },
      "source": [
        "(30 points). This question is designed to help you develop a feel for the way topic modeling works, the connection to the human meanings of documents. Based on the dataset from assignment three, write a python program to **identify the top 10 topics in the dataset**. Before answering this question, please review the materials in lesson 8, especially the code for LDA and LSA. The following information should be reported:\n",
        "\n",
        "(1) Features (top n-gram phrases) used for topic modeling.\n",
        "\n",
        "(2) Top 10 clusters for topic modeling.\n",
        "\n",
        "(3) Summarize and describe the topic for each cluster. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PuFPKhC0m1fd",
        "outputId": "7dcc0668-7d3c-4656-9a12-fcc695985fb7"
      },
      "source": [
        "# Import librariess\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import re, string, nltk, spacy\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn import model_selection\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn import svm\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from gensim import corpora, models, utils\n",
        "\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_6x8wKNBndWv"
      },
      "source": [
        "df = pd.read_csv(\"https://raw.githubusercontent.com/kaziunt2022/assignment_04/main/Reviews.CSV\") # read the reviews file from asignment 3\n",
        "df.dropna(subset=[\"Reviews\"], inplace=True) # drop null rows"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wbn6dmRwndWy"
      },
      "source": [
        "## Cleaning the reviews\n",
        "data = df.Reviews.values.tolist() # Convert each review to list\n",
        "data = [re.sub('\\s+', ' ', sentence) for sentence in data] # remose the line breakers\n",
        "data = [re.sub(\"\\'\",\" \", sentence) for sentence in data] # remocve the \\'\n",
        "\n",
        "def sent_to_words(reviews):\n",
        "    \"\"\"\n",
        "    Input: sentence--> string\n",
        "    Function: Tokenize the sentence and remove punctuations\n",
        "    Output: tokenize and clean reviews\n",
        "    \"\"\"\n",
        "    sentence = []\n",
        "    for review in reviews:\n",
        "        sentence.append(utils.simple_preprocess(str(review).encode('utf-8'), deacc=True))  # deacc=True removes punctuations\n",
        "    return sentence\n",
        "tokenize_reviews = list(sent_to_words(data))"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "620ILVsVndW5",
        "outputId": "9829ac6f-c042-42f8-d5db-c1724c4a7150",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "## bigram and trigam mmodels \n",
        "bigram = models.Phrases(tokenize_reviews, min_count=5, threshold=100) # creat bigram phrases\n",
        "bigram_model = models.phrases.Phraser(bigram) # bigram model\n",
        "trigram_model = models.phrases.Phraser(models.Phrases(bigram[tokenize_reviews], threshold=100))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
            "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1g5RlJ6GndW9"
      },
      "source": [
        "# Define functions for stopwords, bigrams, trigrams and lemmatization\n",
        "stop_words = stopwords.words('english')\n",
        "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])\n",
        "def remove_stopwords(reviews): \n",
        "    \"\"\"\n",
        "    Input: list of lists of reviews\n",
        "    Func: remove all stopwords\n",
        "    Output: tokenize reviews without stop words\n",
        "    \"\"\"\n",
        "    return [[word for word in utils.simple_preprocess(str(review)) if word not in stop_words] for review in reviews]\n",
        "\n",
        "def make_bigrams(reviews):\n",
        "    \"\"\"\n",
        "    Input: tokenize reviews\n",
        "    Func: make bigrams\n",
        "    Output: bigrams of reviews\n",
        "    \"\"\"\n",
        "    return [bigram_model[review] for review in reviews]\n",
        "\n",
        "def make_trigrams(reviews):\n",
        "    \"\"\"\n",
        "    Input: tokenize reviews\n",
        "    Func: make trigrams\n",
        "    Output: trigrams of bigram reviews\n",
        "    \"\"\"\n",
        "    return [trigram_model[bigram_model[review]] for review in reviews]\n",
        "\n",
        "def lemmatization(reviews, allowed=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
        "    \"\"\"\n",
        "    Input: tokenize bigram reviews\n",
        "    Func: return only Noun, adj, verb, adverbs\n",
        "    Output: nouns, adj, verb, adv of reviews\n",
        "    \"\"\"\n",
        "    output_reviews= []\n",
        "    for sent in reviews:\n",
        "        review = nlp(\" \".join(sent)) \n",
        "        output_reviews.append([token.lemma_ for token in review if token.pos_ in allowed])\n",
        "    return output_reviews"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S8FmzwfUndXE"
      },
      "source": [
        "bigrame_reviews = make_bigrams(remove_stopwords(tokenize_reviews)) # take bigram of the Reviews without stopwords\n",
        "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner']) # initiaize the nlp english model\n",
        "lemmatize_reviews = lemmatization(bigrame_reviews, ['NOUN', 'ADJ', 'VERB', 'ADV']) # nouns, adj, verb, adv of reviews"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zrAC1YEHndXL"
      },
      "source": [
        "id2word = corpora.Dictionary(lemmatize_reviews) # Create Dictionary\n",
        "corpus = [id2word.doc2bow(review) for review in lemmatize_reviews] # freq of words"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ZQMuys_ndXO"
      },
      "source": [
        "lda_model = models.ldamodel.LdaModel(corpus=corpus,\n",
        "                                           id2word=id2word,\n",
        "                                           num_topics = 10, \n",
        "                                           random_state=100,\n",
        "                                           update_every=1,\n",
        "                                           chunksize=100,\n",
        "                                           passes=10,\n",
        "                                           alpha='auto',\n",
        "                                           per_word_topics=True) # create LDA model"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N2Wn3EoSndXQ",
        "outputId": "f4fff5e1-fa3e-42ae-fab1-b89d285154bc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "lda_model.print_topics() # print lda topics"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0,\n",
              "  '0.021*\"praise\" + 0.015*\"land\" + 0.012*\"shot\" + 0.012*\"silly\" + 0.011*\"group\" + 0.011*\"guess\" + 0.010*\"sadly\" + 0.009*\"dimension\" + 0.009*\"prove\" + 0.008*\"blend\"'),\n",
              " (1,\n",
              "  '0.016*\"major\" + 0.014*\"narrative\" + 0.009*\"always\" + 0.009*\"white\" + 0.008*\"highlight\" + 0.008*\"leave\" + 0.008*\"franchise\" + 0.007*\"childhood\" + 0.006*\"damn\" + 0.006*\"surely\"'),\n",
              " (2,\n",
              "  '0.019*\"also\" + 0.017*\"get\" + 0.015*\"thing\" + 0.014*\"new\" + 0.014*\"big\" + 0.013*\"villain\" + 0.012*\"say\" + 0.012*\"joke\" + 0.011*\"look\" + 0.011*\"star\"'),\n",
              " (3,\n",
              "  '0.019*\"monster\" + 0.018*\"exactly\" + 0.017*\"disappoint\" + 0.017*\"rest\" + 0.015*\"reveal\" + 0.014*\"camera\" + 0.012*\"high\" + 0.011*\"regard\" + 0.010*\"demon\" + 0.010*\"crazy\"'),\n",
              " (4,\n",
              "  '0.105*\"movie\" + 0.047*\"good\" + 0.036*\"scene\" + 0.034*\"great\" + 0.033*\"marvel\" + 0.030*\"action\" + 0.028*\"story\" + 0.028*\"character\" + 0.027*\"fight\" + 0.027*\"really\"'),\n",
              " (5,\n",
              "  '0.021*\"mother\" + 0.015*\"middle\" + 0.015*\"sister\" + 0.012*\"amount\" + 0.011*\"mystical\" + 0.011*\"child\" + 0.011*\"bus\" + 0.009*\"village\" + 0.009*\"find\" + 0.008*\"wenwu\"'),\n",
              " (6,\n",
              "  '0.028*\"build\" + 0.015*\"entertaining\" + 0.011*\"generally\" + 0.009*\"be\" + 0.008*\"bear\" + 0.008*\"news\" + 0.008*\"impress\" + 0.007*\"alone\" + 0.007*\"please\" + 0.007*\"path\"'),\n",
              " (7,\n",
              "  '0.206*\"film\" + 0.018*\"performance\" + 0.017*\"sequence\" + 0.015*\"predictable\" + 0.009*\"however\" + 0.009*\"well\" + 0.008*\"perfectly\" + 0.007*\"audience\" + 0.007*\"many\" + 0.007*\"average\"'),\n",
              " (8,\n",
              "  '0.024*\"go\" + 0.020*\"make\" + 0.017*\"see\" + 0.014*\"feel\" + 0.012*\"character\" + 0.012*\"would\" + 0.012*\"even\" + 0.011*\"first\" + 0.010*\"chinese\" + 0.010*\"give\"'),\n",
              " (9,\n",
              "  '0.019*\"also\" + 0.014*\"future\" + 0.013*\"power\" + 0.013*\"life\" + 0.012*\"less\" + 0.011*\"help\" + 0.010*\"battle\" + 0.010*\"include\" + 0.010*\"ring\" + 0.009*\"character\"')]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6HkVh6vGndXS"
      },
      "source": [
        "lsa_model = models.LsiModel(corpus, num_topics = 10, id2word = id2word) # create lsa model over corpus"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "owrmrLSkndXU",
        "outputId": "e81f3d06-d1fa-4489-bd53-a7510d56d576",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "lsa_model.print_topics() # print lsa topics with key words"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0,\n",
              "  '0.479*\"movie\" + 0.357*\"film\" + 0.257*\"character\" + 0.199*\"good\" + 0.186*\"scene\" + 0.159*\"marvel\" + 0.155*\"action\" + 0.152*\"great\" + 0.148*\"also\" + 0.145*\"fight\"'),\n",
              " (1,\n",
              "  '-0.697*\"movie\" + 0.617*\"film\" + 0.139*\"also\" + 0.129*\"character\" + 0.122*\"well\" + 0.071*\"great\" + -0.067*\"marvel\" + -0.063*\"good\" + -0.063*\"watch\" + 0.056*\"take\"'),\n",
              " (2,\n",
              "  '0.547*\"film\" + 0.290*\"movie\" + -0.239*\"character\" + -0.207*\"go\" + -0.178*\"also\" + -0.153*\"really\" + -0.129*\"would\" + 0.126*\"marvel\" + -0.124*\"ring\" + -0.120*\"find\"'),\n",
              " (3,\n",
              "  '-0.619*\"good\" + -0.299*\"scene\" + 0.222*\"well\" + -0.203*\"fight\" + 0.183*\"movie\" + -0.169*\"great\" + -0.166*\"also\" + 0.137*\"little\" + 0.136*\"feel\" + 0.129*\"character\"'),\n",
              " (4,\n",
              "  '0.410*\"great\" + 0.313*\"really\" + 0.263*\"well\" + 0.246*\"character\" + -0.234*\"film\" + -0.217*\"go\" + 0.167*\"action\" + 0.143*\"also\" + 0.121*\"scene\" + -0.115*\"good\"'),\n",
              " (5,\n",
              "  '-0.401*\"good\" + 0.386*\"scene\" + -0.351*\"marvel\" + -0.256*\"action\" + -0.248*\"story\" + 0.216*\"fight\" + 0.184*\"feel\" + 0.173*\"film\" + 0.171*\"movie\" + -0.118*\"make\"'),\n",
              " (6,\n",
              "  '-0.504*\"really\" + 0.369*\"scene\" + 0.300*\"fight\" + 0.287*\"action\" + -0.166*\"see\" + 0.165*\"also\" + -0.158*\"go\" + 0.140*\"well\" + -0.118*\"feel\" + -0.117*\"think\"'),\n",
              " (7,\n",
              "  '-0.408*\"great\" + 0.376*\"also\" + -0.288*\"fight\" + -0.256*\"story\" + 0.245*\"character\" + 0.194*\"movie\" + -0.153*\"really\" + 0.143*\"good\" + -0.133*\"feel\" + 0.129*\"action\"'),\n",
              " (8,\n",
              "  '-0.499*\"great\" + 0.320*\"really\" + -0.241*\"see\" + -0.232*\"go\" + 0.215*\"good\" + -0.189*\"action\" + 0.164*\"scene\" + 0.155*\"feel\" + 0.153*\"marvel\" + 0.151*\"make\"'),\n",
              " (9,\n",
              "  '-0.492*\"character\" + 0.324*\"well\" + 0.249*\"good\" + -0.204*\"marvel\" + -0.197*\"scene\" + -0.174*\"new\" + 0.171*\"also\" + 0.151*\"give\" + 0.143*\"little\" + 0.127*\"would\"')]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CZvBSnNXndXW",
        "outputId": "0b1516a9-c8de-4eaa-af76-06ea0bc22bf1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# print out topics visioned by lda and lsa\n",
        "lda_topics =\"\"\"\n",
        "LDA:\n",
        "    1)Familia Series with weapons and min flaw\n",
        "    2)Hope in storyline Theme\n",
        "    3)Expanding Casting\n",
        "    4)Positve anf useful reunion album\n",
        "    5)Attack and Young man's death\n",
        "    6)Fabulous version\n",
        "    7)Adventures Ending\n",
        "    8)Feel of film characters\n",
        "    9)Great marvel movie with story and action\n",
        "    10)Uneccessary steel breaking\"\"\"\n",
        "lsa_topics = \"\"\"\n",
        "LSA:\n",
        "    1)Marveel Movies are great action movies\n",
        "    2)Marvel movie is also well charactered\n",
        "    3)Marevel movies go for real fight\n",
        "    4)Marvel movies has realy great scenes\n",
        "    5)Good and well characted movie\n",
        "    6)Marvel film with action story\n",
        "    7)Good and new action scenes\n",
        "    8)Characters with real fighting scenes\n",
        "    9)Movie is well actioned with great fight scenes\n",
        "    10)Good scenes with new characters\n",
        "    \"\"\"\n",
        "print(lda_topics)\n",
        "print(lsa_topics)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "LDA:\n",
            "    1)Familia Series with weapons and min flaw\n",
            "    2)Hope in storyline Theme\n",
            "    3)Expanding Casting\n",
            "    4)Positve anf useful reunion album\n",
            "    5)Attack and Young man's death\n",
            "    6)Fabulous version\n",
            "    7)Adventures Ending\n",
            "    8)Feel of film characters\n",
            "    9)Great marvel movie with story and action\n",
            "    10)Uneccessary steel breaking\n",
            "\n",
            "LSA:\n",
            "    1)Marveel Movies are great action movies\n",
            "    2)Marvel movie is also well charactered\n",
            "    3)Marevel movies go for real fight\n",
            "    4)Marvel movies has realy great scenes\n",
            "    5)Good and well characted movie\n",
            "    6)Marvel film with action story\n",
            "    7)Good and new action scenes\n",
            "    8)Characters with real fighting scenes\n",
            "    9)Movie is well actioned with great fight scenes\n",
            "    10)Good scenes with new characters\n",
            "    \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfpMRCrRwN6Z"
      },
      "source": [
        "# **Question 2: Sentiment Analysis**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dCQEbDawWCw"
      },
      "source": [
        "(30 points). Sentiment analysis also known as opinion mining is a sub field within Natural Language Processing (NLP) that builds machine learning algorithms to classify a text according to the sentimental polarities of opinions it contains, e.g., positive, negative, neutral. The purpose of this question is to develop a machine learning classifier for sentiment analysis. Based on the dataset from assignment three, write a python program to implement a sentiment classifier and evaluate its performance. Notice: **80% data for training and 20% data for testing**.  \n",
        "\n",
        "(1) Features used for sentiment classification and explain why you select these features.\n",
        "\n",
        "(2) Select two of the supervised learning algorithm from scikit-learn library: https://scikit-learn.org/stable/supervised_learning.html#supervised-learning, to build a sentiment classifier respectively. \n",
        "\n",
        "(3) Compare the performance over accuracy, precision, recall, and F1 score for the two algorithms you selected. Here is the reference of how to calculate these metrics: https://towardsdatascience.com/accuracy-precision-recall-or-f1-331fb37c5cb9. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vATjQNTY8buA"
      },
      "source": [
        "# Write your code here\n",
        "def review_classification(rating):\n",
        "  \"\"\"\n",
        "  This function will return:\n",
        "    very positive for 5\n",
        "    positive for 4\n",
        "    neutral for 3\n",
        "    negative for 2\n",
        "    very negative for 1\n",
        "  \"\"\"\n",
        "    if rating == 5: return 'Very Positive'\n",
        "    elif rating == 4: return 'Positive'\n",
        "    elif rating == 3: return 'Neutral'\n",
        "    elif rating == 2: return 'Negative'\n",
        "    elif rating == 1: return 'Very Negative'\n",
        "\n",
        "df = pd.read_csv(\"https://raw.githubusercontent.com/kaziunt2022/assignment_04/main/Amazon%20Reviews.CSV\")\n",
        "df.dropna(inplace=True, subset=['Rating', \"Reviews\"])\n",
        "df[\"Sentiment\"] = df[\"Rating\"].apply(review_classification)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c3WRz0h_ndXe"
      },
      "source": [
        "stopwords_list = stopwords.words('english') # importing stopwords\n",
        "punctuations_list = string.punctuation # get punctuations\n",
        "lemmatizer = WordNetLemmatizer() # initialize word lemmatizer\n",
        "def preprocessing(text):\n",
        "    \"\"\"\n",
        "    This function will clean the givern text\n",
        "    \"\"\"\n",
        "    text = re.sub('<[^>]*>', '', text)\n",
        "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text)\n",
        "    text = re.sub('[\\W]+', ' ', text.lower())\n",
        "    text = text + \" \".join(emoticons).replace('-', '')\n",
        "    tokenize_text = [lemmatizer.lemmatize(word.lower()) for word in nltk.tokenize.word_tokenize(text) if (word not in stopwords_list) and (word not in punctuations_list) and (len(word)>=2) and (word.isalnum())]\n",
        "    return \" \".join(tokenize_text)\n",
        "\n",
        "df[\"Clean Review\"] = df[\"Reviews\"].apply(preprocessing) # Clean all the reviews got from txt file"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "37-Om1QEndXf"
      },
      "source": [
        "\"\"\"\n",
        "I am using yf_idf feature for it's simple and fast calculation of complex data\n",
        "and getting the the word score efficientlly\n",
        "\"\"\"\n",
        "tf_idf = TfidfVectorizer(ngram_range=(1,2), max_features=1000) # inititate tf_idf model for uni gram to bi gram\n",
        "tf_idf.fit(df['Clean Review']) # apply tf_idf model to cleaned data\n",
        "X = tf_idf.transform(df['Clean Review']) # transfor our data into vector form\n",
        "encoder = LabelEncoder()\n",
        "y = encoder.fit_transform(df['Rating']) # transform ratings\n"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Oi49OkUndXh"
      },
      "source": [
        "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.2) # Split the data aas: 80% training, 20% testing"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xvsVKourndXi",
        "outputId": "1ae36c83-a5ec-400c-a64b-35052ae19f96",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "svm_model = svm.SVC(kernel='linear') # Inititate SVC linear model for supervised modeling\n",
        "svm_model.fit(X_train, y_train) # fit the data to model\n",
        "\n",
        "predicted_y = svm_model.predict(X_test) # perdict the data\n",
        "svm_report = classification_report(y_test, predicted_y, output_dict = True) # get the classification report in dict format\n",
        "\n",
        "print(\"SVM Accuracy: \", accuracy_score(y_test, predicted_y)) # SVM accuracy\n",
        "print(\"Classification Report: \")\n",
        "print(classification_report(y_test, predicted_y)) # print report"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SVM Accuracy:  0.7170273581053491\n",
            "Classification Report: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.68      0.84      0.75       533\n",
            "           1       0.53      0.16      0.24       152\n",
            "           2       0.47      0.23      0.31       178\n",
            "           3       0.49      0.15      0.24       317\n",
            "           4       0.77      0.94      0.85      1269\n",
            "\n",
            "    accuracy                           0.72      2449\n",
            "   macro avg       0.59      0.47      0.48      2449\n",
            "weighted avg       0.68      0.72      0.67      2449\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LJJx0oXendXl",
        "outputId": "02bd5da5-fd74-4329-a91f-b5f94136d9b1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "tree_model = DecisionTreeClassifier() # inititate decision tree model for supervised learning\n",
        "tree_model.fit(X_train, y_train) # fit  the data\n",
        "\n",
        "predicted_y = tree_model.predict(X_test) # perdict the data\n",
        "tree_report = classification_report(y_test, predicted_y, output_dict = True) # get report of perdiction\n",
        "\n",
        "print(\"Decision Tree Model Accuracy: \", accuracy_score(y_test, predicted_y)) # accuracy of the tree model\n",
        "print(\"Classification Report: \")\n",
        "print(classification_report(y_test, predicted_y)) # print out report"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree Model Accuracy:  0.749693752552062\n",
            "Classification Report: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.77      0.77      0.77       533\n",
            "           1       0.62      0.47      0.54       152\n",
            "           2       0.53      0.52      0.52       178\n",
            "           3       0.57      0.49      0.53       317\n",
            "           4       0.82      0.87      0.84      1269\n",
            "\n",
            "    accuracy                           0.75      2449\n",
            "   macro avg       0.66      0.62      0.64      2449\n",
            "weighted avg       0.74      0.75      0.74      2449\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wOiJjhMbndXo",
        "outputId": "12db374f-a86c-40bf-904b-6858f824e7bb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# compare svm results with tree results\n",
        "print(\"SVM Accuracy:\", round(svm_report[\"accuracy\"], 2), '\\tvs\\t', \n",
        "      \"Decision Tree Accuracy:\", round(tree_report[\"accuracy\"], 2))\n",
        "print(\"SVM Prescision:\", round(svm_report[\"weighted avg\"][\"precision\"], 2), '\\tvs\\t', \n",
        "      \"Decision Tree Accuracy:\", round(tree_report[\"weighted avg\"][\"precision\"], 2))\n",
        "print(\"SVM Prescision:\", round(svm_report[\"weighted avg\"][\"recall\"], 2), '\\tvs\\t', \n",
        "      \"Decision Tree Accuracy:\", round(tree_report[\"weighted avg\"][\"recall\"], 2))\n",
        "print(\"SVM Prescision:\", round(svm_report[\"weighted avg\"][\"f1-score\"], 2), '\\tvs\\t', \n",
        "      \"Decision Tree Accuracy:\", round(tree_report[\"weighted avg\"][\"f1-score\"], 2))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SVM Accuracy: 0.72 \tvs\t Decision Tree Accuracy: 0.75\n",
            "SVM Prescision: 0.68 \tvs\t Decision Tree Accuracy: 0.74\n",
            "SVM Prescision: 0.72 \tvs\t Decision Tree Accuracy: 0.75\n",
            "SVM Prescision: 0.67 \tvs\t Decision Tree Accuracy: 0.74\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5mmYIfN8eYV"
      },
      "source": [
        "# **Question 3: House price prediction**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsi2y4z88ngX"
      },
      "source": [
        "(40 points). You are required to build a **regression** model to predict the house price with 79 explanatory variables describing (almost) every aspect of residential homes. The purpose of this question is to practice regression analysis, an supervised learning model. The training data, testing data, and data description files can be download here: https://github.com/unt-iialab/info5731_spring2021/blob/main/assignment/assignment4-question3-data.zip. Here is an axample for implementation: https://towardsdatascience.com/linear-regression-in-python-predict-the-bay-areas-home-price-5c91c8378878. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XfvMKJjIXS5G"
      },
      "source": [
        "# Write your code here\n",
        "train_data = pd.read_csv(\"https://raw.githubusercontent.com/kaziunt2022/assignment_04/main/train.csv\") # train data\n",
        "test_data = pd.read_csv(\"https://raw.githubusercontent.com/kaziunt2022/assignment_04/main/test.csv\") # test data"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uKQLf1PendXv"
      },
      "source": [
        "train_data.fillna(train_data.mean(), inplace = True) # remove null rows\n",
        "test_data.fillna(test_data.mean(), inplace = True) # remove null rows"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ylavdSwQndXy"
      },
      "source": [
        "# encode all the columns in string form to digit form\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "columns = ('GarageCond', 'LandContour', 'RoofStyle', 'RoofMatl', 'Heating', 'MiscFeature', 'SaleType', 'GarageType', 'Electrical', \n",
        "           'SaleCondition', 'Foundation', 'Exterior1st', 'Exterior2nd', 'MasVnrType', 'FireplaceQu', 'LotConfig', 'Neighborhood', \n",
        "           'Condition1', 'Condition2', 'Utilities', 'BldgType', 'HouseStyle','PoolQC', 'BsmtQual', 'BsmtCond', 'GarageQual',\n",
        "           'BsmtExposure', 'ExterQual', 'ExterCond','HeatingQC', 'KitchenQual', 'BsmtFinType1','BsmtFinType2', 'Functional',\n",
        "           'Fence', 'GarageFinish', 'LandSlope','LotShape', 'PavedDrive', 'Street', 'Alley', 'CentralAir', 'MSSubClass',\n",
        "           'OverallCond', 'YrSold', 'MoSold', 'MSZoning')\n",
        "\n",
        "for column in columns:\n",
        "    encoder = LabelEncoder()\n",
        "    train_data[column] = encoder.fit(list(train_data[column].values)).transform(list(train_data[column].values))\n",
        "for column in columns:\n",
        "    encoder = LabelEncoder()\n",
        "    test_data[column] = encoder.fit(list(test_data[column].values)).transform(list(test_data[column].values))"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "654uA_TRndX1"
      },
      "source": [
        "# split the data such data first 80 columns belong to X and the last column will goes to y\n",
        "X_train = train_data[train_data.columns[:80]]\n",
        "y_train = train_data['SalePrice']\n",
        "# store X test data\n",
        "X_test = test_data[test_data.columns[:80]]"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kiQI-V5BndX3",
        "outputId": "23da5551-6965-4b67-c9ab-a47fc1a0ea75",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "rg_model = LinearRegression() # initiate linear regression model\n",
        "rg_model.fit(X_train, y_train) # fit the training data into model"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LinearRegression()"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KIJNUIrpndX6"
      },
      "source": [
        "y_perdicted = rg_model.predict(X_test) # perdict data for test x\n",
        "fill_data = pd.read_csv(\"https://raw.githubusercontent.com/kaziunt2022/assignment_04/main/test.csv\") # import test document for further documentation\n",
        "fill_data[\"Perdicted Price\"] = y_perdicted # store the predicted results\n",
        "fill_data.to_csv(\"Perdicted Prices.CSV\", index=False) # save csv file"
      ],
      "execution_count": 28,
      "outputs": []
    }
  ]
}